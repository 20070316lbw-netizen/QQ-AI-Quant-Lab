# -*- coding: utf-8 -*-
"""
æœç´¢å¼•æ“æ¨¡å— - Search Engine Module
====================================

è´Ÿè´£æ‰§è¡Œç½‘ç»œæœç´¢ä»¥è·å–è´¢ç»æ–°é—»

å®ç°åŸç†:
---------
æ”¯æŒå¤šç§æœç´¢æºï¼Œé»˜è®¤ä¸º DuckDuckGo åŸç”Ÿæœç´¢ã€‚

1. DuckDuckGo (æ¨è): ä½¿ç”¨ duckduckgo-search åº“æ‰§è¡ŒåŸç”Ÿ Python æœç´¢ï¼Œæ— éœ€å¤–éƒ¨ä¾èµ–ã€‚
2. z-ai CLI (å¤‡é€‰): é€šè¿‡ subprocess è°ƒç”¨ z-ai å‘½ä»¤è¡Œå·¥å…·æ‰§è¡Œæœç´¢ã€‚

è¿”å›ç»“æœæ ¼å¼ (JSONæ•°ç»„):
    [
        {
            "url": "https://example.com/news",
            "name": "æ–°é—»æ ‡é¢˜",
            "snippet": "æ–°é—»æ‘˜è¦å†…å®¹...",
            "host_name": "example.com",
            "rank": 0,
            "date": ""
        },
        ...
    ]
"""

import subprocess
import json
from typing import List, Dict, Optional

from .base import (
    SEARCH_SOURCE,
    DEFAULT_TIMEOUT,
    DEFAULT_NUM_RESULTS
)


class SearchEngine:
    """
    æœç´¢å¼•æ“ç±»
    
    ç»Ÿä¸€å°è£…å¤šç§æœç´¢åç«¯æ¥å£
    """
    
    def __init__(self, timeout: int = DEFAULT_TIMEOUT):
        """
        åˆå§‹åŒ–æœç´¢å¼•æ“
        
        Args:
            timeout: æœç´¢è¶…æ—¶æ—¶é—´(ç§’)
        """
        self.timeout = timeout
    
    def search(
        self, 
        query: str, 
        num_results: int = DEFAULT_NUM_RESULTS,
        recency_days: Optional[int] = None
    ) -> List[Dict]:
        """
        æ‰§è¡Œç½‘ç»œæœç´¢
        """
        if SEARCH_SOURCE == "duckduckgo":
            return self._search_duckduckgo(query, num_results, recency_days)
        else:
            return self._search_zai_cli(query, num_results, recency_days)

    def _search_duckduckgo(
        self, 
        query: str, 
        num_results: int,
        recency_days: Optional[int]
    ) -> List[Dict]:
        """ä½¿ç”¨ duckduckgo-search (ddgs) æ‰§è¡Œæœç´¢"""
        try:
            from duckduckgo_search import DDGS
            
            # æ—¶é—´èŒƒå›´æ˜ å°„
            timelimit = None
            if recency_days:
                if recency_days <= 1: timelimit = 'd'
                elif recency_days <= 7: timelimit = 'w'
                elif recency_days <= 30: timelimit = 'm'
                else: timelimit = 'y'

            results = []
            
            # è‡ªåŠ¨è·å–ç³»ç»Ÿä»£ç† (æ”¯æŒ Windows æ³¨å†Œè¡¨ä»£ç†ä¸é€šè¿‡ç¯å¢ƒå˜é‡æ³¨å…¥çš„ä»£ç†)
            import urllib.request
            import os
            
            system_proxies = urllib.request.getproxies()
            proxy_url = system_proxies.get("https") or system_proxies.get("http") or system_proxies.get("all")
            proxy_config = proxy_url if proxy_url else None
            
            # ç”¨äºä¿å­˜è·å–åˆ°çš„åŸå§‹ç»“æœ
            raw_ddgs_results = None
            
            # ç­–ç•¥ï¼šå¦‚æœæ‰¾åˆ°äº†ç³»ç»Ÿä»£ç†ï¼Œå…ˆèµ°ä»£ç†ã€‚å¦‚æœå¤±è´¥ï¼Œå›é€€åˆ°æ— ä»£ç†ï¼ˆç›´è¿ï¼‰ã€‚
            # å¦‚æœä¸€å¼€å§‹å°±æ²¡æœ‰ä»£ç†ï¼Œç›´æ¥èµ°ç›´è¿ã€‚
            if proxy_config:
                print(f"[SearchEngine] å‘ç°ç³»ç»Ÿä»£ç†: {proxy_config}ï¼Œæ­£åœ¨å°è¯•æŒ‚è½½...")
                try:
                    with DDGS(proxy=proxy_config, timeout=self.timeout) as ddgs:
                        raw_ddgs_results = list(ddgs.text(query, max_results=num_results, timelimit=timelimit))
                except Exception as proxy_err:
                    print(f"[SearchEngine] âš ï¸ ä»£ç†è¿æ¥å¤±è´¥ ({proxy_err})ï¼Œæ­£åœ¨å¯åŠ¨ç›´è¿ä¿æŠ¤æœºåˆ¶...")
                    raw_ddgs_results = None
            
            # å¦‚æœå› ä¸ºä¸€å¼€å§‹æ²¡ä»£ç†ï¼Œæˆ–è€…ä½¿ç”¨ä»£ç†å‘ç”Ÿäº†å¤±è´¥ï¼Œå°è¯•æ‰§è¡Œæ— ä»£ç†æ¨¡å¼çš„å¼ºåˆ¶ç›´è¿
            if raw_ddgs_results is None:
                # å°è¯•é€šè¿‡ç¯å¢ƒå˜é‡å±è”½å½±å“åº•å±‚çš„ HTTP_PROXY
                original_http = os.environ.get("HTTP_PROXY")
                original_https = os.environ.get("HTTPS_PROXY")
                if "HTTP_PROXY" in os.environ: del os.environ["HTTP_PROXY"]
                if "HTTPS_PROXY" in os.environ: del os.environ["HTTPS_PROXY"]
                
                try:
                    with DDGS(proxy=None, timeout=self.timeout) as ddgs:
                        raw_ddgs_results = list(ddgs.text(query, max_results=num_results, timelimit=timelimit))
                except Exception as direct_err:
                    print(f"[SearchEngine] âŒ ç›´è¿æœç´¢ä¹Ÿå¤±è´¥äº†: {direct_err}")
                finally:
                    # æ¢å¤å…¨å±€ç¯å¢ƒå˜é‡ï¼Œé¿å…æ±¡æŸ“ Agent å…¶ä»–å±‚çš„è°ƒç”¨é€»è¾‘
                    if original_http is not None: os.environ["HTTP_PROXY"] = original_http
                    if original_https is not None: os.environ["HTTPS_PROXY"] = original_https

            if raw_ddgs_results:
                for i, r in enumerate(raw_ddgs_results):
                    results.append({
                        "url": r.get("href", ""),
                        "name": r.get("title", ""),
                        "snippet": r.get("body", ""),
                        "host_name": r.get("href", "").split("//")[-1].split("/")[0],
                        "rank": i,
                        "date": ""
                    })
            return results
        except Exception as e:
            print(f"[SearchEngine] DuckDuckGo æœç´¢å¼‚å¸¸: {e}")
            return []

    def _search_zai_cli(
        self, 
        query: str, 
        num_results: int,
        recency_days: Optional[int]
    ) -> List[Dict]:
        """è°ƒç”¨ z-ai CLI æ‰§è¡Œæœç´¢ (åŸæœ‰é€»è¾‘)"""
        args = {"query": query, "num": num_results}
        if recency_days:
            args["recency_days"] = recency_days
        
        args_json = json.dumps(args, ensure_ascii=False)
        
        try:
            result = subprocess.run(
                [CLI_COMMAND, "function", "-n", CLI_FUNCTION_NAME, "-a", args_json],
                capture_output=True,
                text=True,
                timeout=self.timeout
            )
            
            if result.returncode != 0:
                print(f"[SearchEngine] CLI æœç´¢é”™è¯¯: {result.stderr}")
                return []
            
            return self._parse_cli_output(result.stdout)
            
        except subprocess.TimeoutExpired:
            print("[SearchEngine] CLI æœç´¢è¶…æ—¶")
            return []
        except FileNotFoundError:
            print("[SearchEngine] æœªæ‰¾åˆ° z-ai å‘½ä»¤ï¼Œä¸” SEARCH_SOURCE è®¾ç½®ä¸º z-ai")
            return []
        except Exception as e:
            print(f"[SearchEngine] CLI æœç´¢å¼‚å¸¸: {e}")
            return []
    
    def _parse_cli_output(self, output: str) -> List[Dict]:
        """
        è§£æCLIè¾“å‡ºï¼Œæå–JSONæ•°æ®
        
        CLIè¾“å‡ºæ ¼å¼:
        ğŸš€ Initializing Z-AI SDK...
        ğŸš€ Invoking function: web_search...
        [
            {...},
            {...}
        ]
        ğŸ‰ Function invocation completed!
        
        Args:
            output: CLIåŸå§‹è¾“å‡º
            
        Returns:
            è§£æåçš„æœç´¢ç»“æœåˆ—è¡¨
        """
        # æŸ¥æ‰¾JSONæ•°ç»„è¾¹ç•Œ
        json_start = output.find('[')
        json_end = output.rfind(']') + 1
        
        if json_start == -1 or json_end == 0:
            print("[SearchEngine] æœªæ‰¾åˆ°æœ‰æ•ˆçš„JSONæ•°æ®")
            return []
        
        json_str = output[json_start:json_end]
        results = json.loads(json_str)
        
        return results if isinstance(results, list) else []
    
    def search_multiple(
        self, 
        queries: List[str], 
        num_per_query: int = DEFAULT_NUM_RESULTS,
        recency_days: Optional[int] = None
    ) -> Dict[str, List[Dict]]:
        """
        æ‰¹é‡æœç´¢å¤šä¸ªå…³é”®è¯
        
        Args:
            queries: å…³é”®è¯åˆ—è¡¨
            num_per_query: æ¯ä¸ªå…³é”®è¯çš„ç»“æœæ•°é‡
            recency_days: æ—¶é—´èŒƒå›´
            
        Returns:
            å­—å…¸ï¼Œkeyä¸ºå…³é”®è¯ï¼Œvalueä¸ºæœç´¢ç»“æœåˆ—è¡¨
        """
        results = {}
        for query in queries:
            print(f"[SearchEngine] æœç´¢: {query}")
            results[query] = self.search(query, num_per_query, recency_days)
        return results
